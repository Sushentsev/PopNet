raw_data: 'data/gensongs/'
train_data: 'train/preprocess/gpt_data/'
device: 'cuda'

model:
  path: 'train/models/gpt'
  attention-dropout: 0.1
  num-attention-heads: 16
  hidden-size: 1024
  intermediate-size: None
  num-layers: 24
  layernorm-epsilon: 1e-5
  hidden-dropout: 0.1
  max-position-embeddings: 512
  vocab-size: 30522
  deep-init: False
  make-vocab-size-divisible-by: 8
  cpu-optimizer: False
  cpu_torch_adam: False
  sparse-mode: False

fp16:
  fp16: False
  fp32-embedding: False
  fp32-layernorm: False
  fp32-tokentypes: False
  fp32-allreduce: False
  hysteresis: 2
  loss-scale: None
  loss-scale-window: 1000
  min-scale: 1

train:
  batch-size: 4
  weight-decay: 0.01
  checkpoint-activations: False
  checkpoint-num-layers: 1
  deepspeed-activation-checkpointing: False
  clip-grad: 1.0
  train-iters: 1000000
  log-interval: 100
  logging-dir: None
  exit-interval: None
  seed: 1234
  reset-position-ids: False
  reset-attention-mask: False
  lr-decay-iters: None
  lr-decay-style: 'linear'
  lr: 1.0e-4
  min-lr: 1.0e-6
  warmup: 0.01
  save: None
  save-interval: 5000
  no-save-optim: False
  no-save-rng: False
  load: None
  no-load-optim: False
  log-memory: False
  no-load-rng: False
  load-huggingface: None
  export-huggingface: None
  huggingface-double-pos-embeddings: False
  load-tag: ''
  cache-prefix: '_'
  finetune: False
  resume-dataloader: False
  distributed-backend: 'nccl'
  local_rank: None

validation:
  eval-batch-size: None
  eval-iters: 100
  eval-interval: 1000
  eval-seq-length: None
  eval-max-preds-per-seq: None
  overlapping-eval: 32
  cloze-eval: False
  eval-hf: False
  load-openai: False

data:
  model-parallel-size: 1
  shuffle: False
  train-data: None
  use-npy-data-loader: False
  train-data-path: ''
  val-data-path: ''
  test-data-path: ''
  input-data-sizes-file: 'sizes.txt'
  delim: ', '
  text-key: 'sentence'
  eval-text-key: None
  valid-data: None
  split: '1000,1,1'
  test-data: None
  overwrite-cache: False
  lazy-loader: False
  loose-json: False
  presplit-sentences: False
  num-workers: 2
  tokenizer-path: None
  cache-dir: None
  use-tfrecords: False
  seq-length: 512
  max-files-per-process: 50000
  max-preds-per-seq: None

configurations:
  temperature: 1.0
  top_p: 0.0
  top_k: 0
  out-seq-length: 256
  tg-token-name: 'token.txt'



